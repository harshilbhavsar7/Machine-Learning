{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D,Activation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen=ImageDataGenerator(rotation_range=30,#rotate the image 30 degrees\n",
    "                             width_shift_range=0.1,#shift the pic width by amax of 10%\n",
    "                             height_shift_range=0.1,#shift the pic height by amax of 10%\n",
    "                             rescale=1/255,#rescale tghe image by normalizing it\n",
    "                             shear_range=0.2,#shear means cutting away part of the image(max 20%)\n",
    "                             zoom_range=0.2,# Zoom in by 20% max\n",
    "                             horizontal_flip=True,#allow horizontal flipping\n",
    "                             fill_mode='nearest',#fill in  missing pixel with the nearest filled value\n",
    "                             validation_split=0.2\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# width,height,channel\n",
    "image_shape=(150,150,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),input_shape=(150,150,3),activation='relu',))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),input_shape=(150,150,3),\n",
    "                activation='relu',))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=128,kernel_size=(3,3),input_shape=(150,150,3),\n",
    "                activation='relu',))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=128,kernel_size=(3,3),input_shape=(150,150,3),\n",
    "                activation='relu',))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "#DropOuts help reduce overfitting by randomly turning neurons off during training.\n",
    "# Here we say randomly turn off 50% of the Neurons\n",
    "\n",
    "\n",
    "# Last layer for 0=paper and 1=rock and 2=scissors\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',#Computes the cross-entropy loss between true labels and predicted labels.\n",
    "             optimizer='rmsprop',#The RMSprop optimizer is similar to the gradient descent algorithm with momentum. The RMSprop optimizer restricts the oscillations in the vertical direction. Therefore, we can increase our learning rate and our algorithm could take larger steps in the horizontal direction converging faster\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 148, 148, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 74, 74, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 72, 72, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 1539      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 3,473,475\n",
      "Trainable params: 3,473,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2016 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_image_gen=image_gen.flow_from_directory(r'C:\\Users\\Harshil\\Downloads\\rps',target_size=image_shape[:2],\n",
    "                                             class_mode='categorical',\n",
    "                                             subset='training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 504 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "validation_image_gen=image_gen.flow_from_directory(r'C:\\Users\\Harshil\\Downloads\\rps',target_size=image_shape[:2],\n",
    "                                             class_mode='categorical',\n",
    "                                             subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 372 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_image_gen=image_gen.flow_from_directory(r'C:\\Users\\Harshil\\Downloads\\rps-test-set',target_size=image_shape[:2],\n",
    "                                             class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper': 0, 'rock': 1, 'scissors': 2}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "63/63 [==============================] - 271s 4s/step - loss: 1.1679 - accuracy: 0.5149 - val_loss: 0.8713 - val_accuracy: 0.4922\n",
      "Epoch 2/15\n",
      "63/63 [==============================] - 164s 3s/step - loss: 0.6684 - accuracy: 0.7460 - val_loss: 0.5383 - val_accuracy: 0.7617\n",
      "Epoch 3/15\n",
      "63/63 [==============================] - 162s 3s/step - loss: 0.3835 - accuracy: 0.8472 - val_loss: 0.5495 - val_accuracy: 0.6992\n",
      "Epoch 4/15\n",
      "63/63 [==============================] - 170s 3s/step - loss: 0.2462 - accuracy: 0.9082 - val_loss: 0.5021 - val_accuracy: 0.7930\n",
      "Epoch 5/15\n",
      "63/63 [==============================] - 162s 3s/step - loss: 0.1708 - accuracy: 0.9449 - val_loss: 0.1914 - val_accuracy: 0.9336\n",
      "Epoch 6/15\n",
      "63/63 [==============================] - 165s 3s/step - loss: 0.1265 - accuracy: 0.9588 - val_loss: 1.1483 - val_accuracy: 0.6445\n",
      "Epoch 7/15\n",
      "63/63 [==============================] - 164s 3s/step - loss: 0.1099 - accuracy: 0.9583 - val_loss: 0.5453 - val_accuracy: 0.8398\n",
      "Epoch 8/15\n",
      "63/63 [==============================] - 169s 3s/step - loss: 0.1039 - accuracy: 0.9643 - val_loss: 0.2636 - val_accuracy: 0.9414\n",
      "Epoch 9/15\n",
      "63/63 [==============================] - 166s 3s/step - loss: 0.0840 - accuracy: 0.9762 - val_loss: 0.1295 - val_accuracy: 0.9492\n",
      "Epoch 10/15\n",
      "63/63 [==============================] - 162s 3s/step - loss: 0.0828 - accuracy: 0.9767 - val_loss: 0.2130 - val_accuracy: 0.9180\n",
      "Epoch 11/15\n",
      "63/63 [==============================] - 174s 3s/step - loss: 0.0592 - accuracy: 0.9812 - val_loss: 0.7637 - val_accuracy: 0.7422\n",
      "Epoch 12/15\n",
      "63/63 [==============================] - 175s 3s/step - loss: 0.0837 - accuracy: 0.9777 - val_loss: 0.9369 - val_accuracy: 0.8125\n",
      "Epoch 13/15\n",
      "63/63 [==============================] - 169s 3s/step - loss: 0.0778 - accuracy: 0.9831 - val_loss: 0.2936 - val_accuracy: 0.8867\n",
      "Epoch 14/15\n",
      "63/63 [==============================] - 166s 3s/step - loss: 0.0557 - accuracy: 0.9826 - val_loss: 1.0933 - val_accuracy: 0.7578\n",
      "Epoch 15/15\n",
      "63/63 [==============================] - 168s 3s/step - loss: 0.0748 - accuracy: 0.9762 - val_loss: 0.1799 - val_accuracy: 0.9414\n"
     ]
    }
   ],
   "source": [
    "results=model.fit_generator(train_image_gen,epochs=15,\n",
    "                           validation_data=validation_image_gen,\n",
    "                            validation_steps=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('rps3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 10s 804ms/step - loss: 0.1083 - accuracy: 0.9651\n",
      "Loss:  0.1082633084927996\n",
      "Accuracy:  0.96505374\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_image_gen)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 9s - loss: 0.0773 - accuracy: 0.9624\n"
     ]
    }
   ],
   "source": [
    "results_2 = model.evaluate(test_image_gen, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
